{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "from typing import Union\n",
    "from glob import glob\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Project\n",
    "\n",
    "\n",
    "**Group ID:** xx\n",
    "\n",
    "**Author 1 (sciper):** Student Name 1 (xxxxx)  \n",
    "**Author 2 (sciper):** Student Name 2 (xxxxx)   \n",
    "**Author 3 (sciper):** Student Name 3 (xxxxx)   \n",
    "\n",
    "**Release date:** 27.04.2023\n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The assignments are designed to teach practical implementation of the topics presented during class as well as preparation for the final project, which is a practical project which ties together the topics of the course. \n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook !** rerun the notebook from scratch `Kernel` > `Restart & Run All`\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Introduction\n",
    "\n",
    "In this project, you will be working on solving tiling puzzles using image analysis and pattern recognition techniques. Tiling puzzles are a classic type of puzzle game that consists of fitting together pieces of a given shape (in this case squared to form a complete image. The goal of this project is to develop an algorithm that can automatically reconstruct tiling puzzles from a single input image. \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data\n",
    "\n",
    "### Input data\n",
    "To achieve your task, you will be given images that look like this:\n",
    "\n",
    "\n",
    "![train_00.png](data_project/project_description/train_00.png)\n",
    "### Example puzzle content\n",
    "Example of input of solved puzzles. \n",
    "Solution 1\n",
    "<img src=\"data_project/project_description/solution_example.png\" width=\"512\"/>\n",
    "\n",
    "Solution 2\n",
    "<img src=\"data_project/project_description/solution_example2.jpg\" width=\"512\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Image layout\n",
    "\n",
    "- The input for the program will be a single image with a size of __2000x2000 pixels__, containing the pieces of the tiling puzzles randomly placed in it. The puzzles sizes vary from __3x3, 3x4, or 4x4__ size. \n",
    "    -__You are guaranteed to always have the exact number of pieces for each puzzle__ \n",
    "        -For each puzzle you always are expected to find exaclty 9,12,16 pieces\n",
    "        -If you find something else, either you are missing pieces, or added incorrect pieces for the puzzle\n",
    "\n",
    "- The puzzle pieces are square-shaped with dimensions of 128x128 pixels (before rotation). \n",
    "\n",
    "- The input image will contain pieces from __two or three (but never four)__ different tiling puzzles, as well as some __extra pieces (outliers)__ that do not belong to either puzzle.\n",
    "\n",
    "\n",
    "## 2. Tasks (Total 20 points) \n",
    "\n",
    "\n",
    "The project aims to:\n",
    "1) Segment the puzzle pieces from the background (recover the pieces of 128x128 pixels)   \\[ __5 points__ \\] \n",
    "\n",
    "2) Extract features of interest from puzzle pieces images \\[ __5 points__ \\]   \n",
    "\n",
    "3) Cluster puzzle pieces to identify which puzzle they belong, and identify outliers.  \\[ __5 points__ \\]   \n",
    "\n",
    "4) Solve tiling puzzle (find the rotations and translations to correctly allocate the puzzle pieces in a 3x3, 3x4 or 4x4 array.) \\[ __5 points__ \\]   \n",
    "\n",
    "##### The images used for the puzzles have self-repeating patterns or textures, which ensures that all puzzle pieces contain more or less the same features regardless of how they were cut. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2. Output solution pieces.\n",
    "\n",
    "For each inpute image, the output solution will include N images with solved puzzles, where N is the number of puzzles in the input image. and M images, that are Each of these images will contain the solved solution to one of the N puzzles in the input. \n",
    "\n",
    "\n",
    "-  Example input:  train_05.png\n",
    "\n",
    "- Example solution:\n",
    "        -solution_05_00.png solution_05_01.png solution_05_02.png \n",
    "        -outlier_05_00.png outlier_05_01.png outlier_05_02.png ...\n",
    "\n",
    "- Example input:  train_07.png\n",
    "- Example solution:\n",
    "        -solution_07_00.png solution_07_01.png \n",
    "        -outlier_07_00.png outlier_07_01.png outlier_07_02.png ...\n",
    "\n",
    "\n",
    "__Watch out!__ output resolution should always be like this:  \n",
    "<table ><tr><th >Puzzle pieces <th><th> pixel dimentions <th> <th> pixel dimentions <th> <tr>\n",
    "<tr><td> 3x3 <td><td> 384x384 <td><td> 3(128)x3(128) <td> <tr>\n",
    "<tr><td> 3x4 <td><td> 384x512 <td><td> 3(128)x4(128)<tr>\n",
    "<tr><td> 4x4 <td><td> 512x512 <td><td> 4(128)x4(128)<tr>\n",
    "<tr><td> 1x1 (outlier)<td><td> 128x128 <td><td> (1)128x(1)128 <td><tr><table>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Order of the solutions (and rotations) it's not a problem for the grading__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "the output solution will be a final image of resolution (1283)x(1283), with each piece correctly placed in its corresponding location in the 3x3 array. Similarly, if the puzzle consists of 3x4 or 4x4 pieces, the output solution will be an image of resolution (1283)x(1284) or (1284)x(1284)\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 Data folder Structure\n",
    "\n",
    "You can download the data for the project here: [download data](https://drive.google.com/drive/folders/1k3xTH0ZhpqZb3xcZ6wsOSjLzxBNYabg3?usp=share_link)\n",
    "\n",
    "```\n",
    "data_project\n",
    "│\n",
    "└─── project_description\n",
    "│    │    example_input.png      # example input images\n",
    "│    │    example_textures1.png      # example input images\n",
    "│    │    example_textures2.png      # example input images\n",
    "│    └─── ultimate_test.jpg   # If it works on that image, you would probably end up with a good score\n",
    "│\n",
    "└─── train\n",
    "│    │    train_00.png        # Train image 00\n",
    "│    │    ...\n",
    "│    │    train_16.png        # Train image 16\n",
    "│    └─── train_labels.csv    # Ground truth of the train set\n",
    "|    \n",
    "└────train_solution\n",
    "│    │    solution_00_00.png        # Solution puzzle 1 from Train image 00\n",
    "│    │    solution_00_01.png        # Solution puzzle 2 from Train image 00\n",
    "│    │    solution_00_02.png        # Solution Puzzle 3 from Train image 00\n",
    "│    │    outlier_00_00.png         # outlier     from Train image 00\n",
    "│    │    outlier_00_01.png         # outlier     from Train image 00\n",
    "│    │    outlier_00_03.png         # outlier     from Train image 00\n",
    "│    │    ...\n",
    "│    │    solution_15_00.png        # Solution puzzle 1 from Train image 15\n",
    "│    │    solution_15_01.png        # Solution puzzle 2 from Train image 15\n",
    "│    │    outlier_15_00.png         # outlier     from Train image 15\n",
    "│    └─── outlier_15_01.png         # outlier     from Train image 15\n",
    "│\n",
    "└─── test\n",
    "     │    test_00.png         # Test image 00 (day of the exam only)\n",
    "     │    ...\n",
    "     └─── test_xx.png             # Test image xx (day of the exam only)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Evaluation\n",
    "\n",
    "**Before the exam**\n",
    "   - Create a zipped folder named **groupid_xx.zip** that you upload on moodle (xx being your group number).\n",
    "   - Include a **runnable** code (Jupyter Notebook and external files) and your presentation in the zip folder.\n",
    "   \n",
    "**The day of the exam**\n",
    "   - You will be given a **new folder** (test folder) with few images, but **no ground truth** (no solutions).\n",
    "   - We will ask you to run your pipeline in **real time** and to send us your prediction of the task you obtain with the provided function **save_results**. \n",
    "   - On our side, we will compute the performance of your classification algorithm. \n",
    "   - To evaluate your method, we will use the **evaluate_solution** function presented below. To understand how the provided functions work, please read the documentation of the functions in **utils.py**.\n",
    "   - **Please make sure your function returns the proper data format to avoid points penalty on the day of the exam**. \n",
    "---\n",
    "\n",
    "\n",
    "## 4. Segment The puzzle with the background"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Imports and load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load images\n",
    "import os \n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from skimage.filters import threshold_local\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.color import rgb2hsv\n",
    "from skimage.color import rgb2hsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_input_image(image_index ,  folder =\"train2\" , path = \"data_project\"):\n",
    "    \n",
    "    filename = \"train_{}.png\".format(str(image_index).zfill(2))\n",
    "    path_solution = os.path.join(path,folder , filename )\n",
    "    \n",
    "    im= Image.open(os.path.join(path,folder,filename)).convert('RGB')\n",
    "    im = np.array(im)\n",
    "    return im\n",
    "\n",
    "def save_solution_puzzles(image_index , solved_puzzles, outliers  , folder =\"train\" , path = \"data_project\"  ,group_id = 0):\n",
    "    \n",
    "    path_solution = os.path.join(path,folder + \"_solution_{}\".format(str(group_id).zfill(2)))\n",
    "    if not  os.path.isdir(path_solution):\n",
    "        os.mkdir(path_solution)\n",
    "\n",
    "    print(path_solution)\n",
    "    for i, puzzle in enumerate(solved_puzzles):\n",
    "        filename =os.path.join(path_solution, \"solution_{}_{}.png\".format(str(image_index).zfill(2), str(i).zfill(2)))\n",
    "        Image.fromarray(puzzle).save(filename)\n",
    "\n",
    "    for i , outlier in enumerate(outliers):\n",
    "        filename =os.path.join(path_solution, \"outlier_{}_{}.png\".format(str(image_index).zfill(2), str(i).zfill(2)))\n",
    "        Image.fromarray(outlier).save(filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train = 11\n",
    "train_image = []\n",
    "i = 0\n",
    "while i <= nb_train:\n",
    "    train_image.append(load_input_image(i))\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Edge detection & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_detector(image):\n",
    "    # Convert the image to grayscale\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Blur the image for better edge detection\n",
    "    img_blur = cv2.GaussianBlur(img_gray, (3,3), 0) \n",
    "\n",
    "    # Canny Edge Detection\n",
    "    edges = cv2.Canny(image=img_blur, threshold1=0.5, threshold2=20) # Canny Edge Detection\n",
    "\n",
    "\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(edge,min_size=600) :\n",
    "\n",
    "    #Perform morphological closing to fill small holes\n",
    "    # Start with a Cloing \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (12, 12))\n",
    "    closed = cv2.morphologyEx(edge, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "\n",
    "    # Find connected components in the binary image\n",
    "    _, labels, stats, _ = cv2.connectedComponentsWithStats(closed, connectivity=8)\n",
    "\n",
    "    # Filter out small objects based on their area\n",
    "    filtered = np.zeros_like(closed)\n",
    "    for label in range(1, len(stats)):\n",
    "        area = stats[label, cv2.CC_STAT_AREA]\n",
    "        if area >= min_size:\n",
    "            filtered[labels == label] = 255\n",
    "\n",
    "    # Apply the filtered mask to the original image\n",
    "    result = cv2.bitwise_and(closed, closed, mask=filtered)\n",
    "\n",
    "    # Apply 2 Dilation \n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    result = cv2.dilate(result, kernel, iterations=2)\n",
    "\n",
    "    #Opening--> Closing again\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    result = cv2.morphologyEx(result, cv2.MORPH_OPEN, kernel)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    closed = cv2.morphologyEx(result, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    \n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the edge detector follow by the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_image = []\n",
    "i = 0\n",
    "while i <= nb_train:\n",
    "    edge_image.append(pre_processing( edge_detector(train_image[i])))\n",
    "    i += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the contours from de preprocessed mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contours =[]\n",
    "nb_piece =[]\n",
    "image_contour = copy.deepcopy(train_image)\n",
    "for i in range(nb_train) :\n",
    "    contours, _ = cv2.findContours(edge_image[i], cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    nb_piece.append(len(contours))\n",
    "    all_contours.append(contours)\n",
    "    cv2.drawContours(image_contour[i], all_contours[i], -1, (255, 0, 0), 3)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montrer les images avec les contours trouvés, montrer à côté les mask associés utilisé pour trouver les contours :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(11, 2, figsize=(25, 150))\n",
    "\n",
    "\n",
    "for i in range(11):\n",
    "    ax[i][0].imshow(image_contour[i])\n",
    "    ax[i][0].title.set_text(f\"Image {i} avec les contours trouvés\")\n",
    "    ax[i][1].imshow(edge_image[i])\n",
    "    ax[i][1].title.set_text(f\"Mask associé à l'image {i}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 : Extract the pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_piece(nb_train,all_contours,nb_piece) :\n",
    "\n",
    "    extracted_piece= []\n",
    "\n",
    "    for t in range(nb_train): #iterate on all train picture\n",
    "        extracted_piece.append([])\n",
    "        for i in range(nb_piece[t]) : #iterate on all piece found\n",
    "\n",
    "            contour = all_contours[t][i]\n",
    "            # Approximation du contour par un polygone\n",
    "            epsilon = 0.01 * cv2.arcLength(contour, True)\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "            # Calculer le rectangle orienté minimum (ROT)\n",
    "            rect = cv2.minAreaRect(approx)\n",
    "\n",
    "            # Extraire le carré de 128 pixels\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.int0(box)\n",
    "            width = int(rect[1][0])\n",
    "            height = int(rect[1][1])\n",
    "\n",
    "            #Skip if len and width dont correspond to the dimensions\n",
    "            if width <115 or height <115 :\n",
    "                print(f\"The piece {i} in puzzle {t} Skipped, it is to small\")\n",
    "                continue\n",
    "            \n",
    "\n",
    "            \n",
    "            src_pts = box.astype(\"float32\")\n",
    "            dst_pts = np.array([[0, height - 1], [0, 0], [width - 1, 0], [width - 1, height - 1]], dtype=\"float32\")\n",
    "            M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "            warped = cv2.warpPerspective(train_image[t], M, (width, height))\n",
    "            warped = cv2.resize(warped, (128, 128))\n",
    "\n",
    "\n",
    "            extracted_piece[t].append(warped)\n",
    "        \n",
    "        nb_piece[t] = len(extracted_piece[t])\n",
    "\n",
    "    return extracted_piece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_piece = extract_all_piece(nb_train,all_contours,nb_piece)\n",
    "print(\"Nombre de pieces par Image : \",nb_piece)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a specific extracted piece : [image][number of the piece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(extracted_piece[6][2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all extracted piece from 1 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =8\n",
    "fig, ax = plt.subplots(nb_piece[test], figsize=(25, 150))\n",
    "\n",
    "for i in range(nb_piece[test]) :\n",
    "    ax[i].imshow(extracted_piece[test][i])\n",
    "    #ax[i].title.set_text(f\"Piece  {i}, Contour size :{len(contour)} \")\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5 : Extract features "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourrier descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import feature\n",
    "\n",
    "def extract_lbp_features2(image_list):\n",
    "    lbp_features = []\n",
    "    \n",
    "    for image in image_list:\n",
    "        # Convert the RGB image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Extract LBP features\n",
    "        lbp = feature.local_binary_pattern(gray_image, 16, 6, method='uniform')\n",
    "        \n",
    "        # Calculate the histogram of LBP features\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 257), range=(0, 256))\n",
    "        \n",
    "        # Normalize the histogram using z-normalization\n",
    "        hist_mean = np.mean(hist)\n",
    "        hist_std = np.std(hist)\n",
    "        normalized_hist = (hist - hist_mean) / (hist_std + 1e-7)\n",
    "        \n",
    "        # Append the normalized LBP histogram to the feature list\n",
    "        lbp_features.append(normalized_hist)\n",
    "    \n",
    "    # Stack all the LBP histogram features into a single array\n",
    "    lbp_features = np.stack(lbp_features)\n",
    "    \n",
    "    # Normalize the features using z-normalization\n",
    "    lbp_features = (lbp_features - np.mean(lbp_features, axis=0)) / (np.std(lbp_features, axis=0) + 1e-7)\n",
    "    \n",
    "    # Select the three most valuable features (assuming the shape of lbp_features is (num_samples, num_bins))\n",
    "    top_features_indices = np.argsort(np.mean(lbp_features, axis=0))[::-1][:3]\n",
    "    top_features = lbp_features[:, top_features_indices]\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "\n",
    "def extract_lbp_features(image_list, num_components=3):\n",
    "    lbp_features = []\n",
    "    \n",
    "    for image in image_list:\n",
    "        # Convert the RGB image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Extract LBP features\n",
    "        lbp = feature.local_binary_pattern(gray_image, 16, 6, method='uniform')\n",
    "        \n",
    "        # Calculate the histogram of LBP features\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 257), range=(0, 256))\n",
    "        \n",
    "        # Normalize the histogram using z-normalization\n",
    "        hist_mean = np.mean(hist)\n",
    "        hist_std = np.std(hist)\n",
    "        normalized_hist = (hist - hist_mean) / (hist_std + 1e-7)\n",
    "        \n",
    "        # Append the normalized LBP histogram to the feature list\n",
    "        lbp_features.append(normalized_hist)\n",
    "    \n",
    "    # Stack all the LBP histogram features into a single array\n",
    "    lbp_features = np.stack(lbp_features)\n",
    "    \n",
    "    # Normalize the features using z-normalization\n",
    "    lbp_features = (lbp_features - np.mean(lbp_features, axis=0)) / (np.std(lbp_features, axis=0) + 1e-7)\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=num_components)\n",
    "    reduced_features = pca.fit_transform(lbp_features)\n",
    "    \n",
    "    return reduced_features\n",
    "\n",
    "\n",
    "def extract_lpq_features(image_list):\n",
    "    lpq_features = []\n",
    "    \n",
    "    for image in image_list:\n",
    "        # Convert the RGB image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Apply LPQ\n",
    "        lpq = lpq_operator(gray_image)\n",
    "        \n",
    "        # Calculate the histogram of LPQ features\n",
    "        hist, _ = np.histogram(lpq.ravel(), bins=np.arange(0, 257), range=(0, 256))\n",
    "        \n",
    "        # Normalize the histogram using z-normalization\n",
    "        hist_mean = np.mean(hist)\n",
    "        hist_std = np.std(hist)\n",
    "        normalized_hist = (hist - hist_mean) / (hist_std + 1e-7)\n",
    "        \n",
    "        # Append the normalized LPQ histogram to the feature list\n",
    "        lpq_features.append(normalized_hist)\n",
    "    \n",
    "    # Stack all the LPQ histogram features into a single array\n",
    "    lpq_features = np.stack(lpq_features)\n",
    "    \n",
    "    # Normalize the features using z-normalization\n",
    "    lpq_features = (lpq_features - np.mean(lpq_features, axis=0)) / (np.std(lpq_features, axis=0) + 1e-7)\n",
    "    \n",
    "    # Select the three most valuable features (assuming the shape of lpq_features is (num_samples, num_bins))\n",
    "    top_features_indices = np.argsort(np.mean(lpq_features, axis=0))[::-1][:3]\n",
    "    top_features = lpq_features[:, top_features_indices]\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "\n",
    "def lpq_operator(image):\n",
    "    # LPQ parameters\n",
    "    radius = 6\n",
    "    points = 10\n",
    "    \n",
    "    # Apply LPQ operator\n",
    "    height, width = image.shape\n",
    "    lpq = np.zeros((height - 2 * radius, width - 2 * radius), dtype=np.uint8)\n",
    "    for i in range(radius, height - radius):\n",
    "        for j in range(radius, width - radius):\n",
    "            center_value = image[i, j]\n",
    "            code = 0\n",
    "            for k in range(points):\n",
    "                x = int(round(i + radius * np.cos(2 * np.pi * k / points)))\n",
    "                y = int(round(j - radius * np.sin(2 * np.pi * k / points)))\n",
    "                if image[x, y] >= center_value:\n",
    "                    code += 1 << k\n",
    "            lpq[i - radius, j - radius] = code\n",
    "    \n",
    "    return lpq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_gabor_features(images, num_filters=8, frequencies=[0.1, 0.2, 0.3], orientations=[0,np.pi/4, np.pi/2, 3*np.pi/4]):\n",
    "    preprocessed_images = images  # You need to define this preprocessing step\n",
    "    feature_vectors = []\n",
    "\n",
    "    for img in preprocessed_images:\n",
    "        filtered_responses = []\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        for frequency in frequencies:\n",
    "            for orientation in orientations:\n",
    "                # Create a Gabor kernel with the specified parameters\n",
    "                gabor_kernel = cv2.getGaborKernel((25, 25), 2, orientation, frequency, 0.2, 0, ktype=cv2.CV_32F)\n",
    "\n",
    "                # Apply the Gabor filter to the grayscale image\n",
    "                filtered_img = cv2.filter2D(gray_img, cv2.CV_32F, gabor_kernel)\n",
    "                # Store the filtered response in a list\n",
    "                filtered_responses.append(filtered_img)\n",
    "\n",
    "        # Flatten the filtered responses into a feature vector\n",
    "        feature_vector = np.concatenate(filtered_responses).ravel()\n",
    "        feature_vectors.append(feature_vector)\n",
    "\n",
    "    # Convert the list to a NumPy array\n",
    "    feature_vectors = np.array(feature_vectors)\n",
    "\n",
    "    # Apply feature normalization (e.g., z-score normalization)\n",
    "    feature_vectors = (feature_vectors - np.mean(feature_vectors, axis=0)) / np.std(feature_vectors, axis=0)\n",
    "\n",
    "    # Extract Gabor features\n",
    "\n",
    "    # Select the top-k filter responses with the highest importance scores\n",
    "    top_k = 3\n",
    "    selected_responses = select_top_filter_responses(feature_vectors, top_k)\n",
    "\n",
    "\n",
    "    return selected_responses\n",
    "\n",
    "\n",
    "def calculate_filter_importance(feature_vectors):\n",
    "    # Calculate the variance of each filter response\n",
    "    variances = np.var(feature_vectors, axis=0)\n",
    "\n",
    "    # Sort the variances in descending order and get the corresponding indices\n",
    "    sorted_indices = np.argsort(variances)[::-1]\n",
    "\n",
    "    return sorted_indices\n",
    "\n",
    "def select_top_filter_responses(feature_vectors, top_k):\n",
    "    # Calculate the importance/relevance of each filter response\n",
    "    importance_scores = calculate_filter_importance(feature_vectors)\n",
    "\n",
    "    # Select the top-k filter responses with the highest importance scores\n",
    "    top_responses = feature_vectors[:, importance_scores[:top_k]]\n",
    "\n",
    "    return top_responses\n",
    "\n",
    "\n",
    "\n",
    "def extract_hue_indicator(images):\n",
    "    hue_features = []\n",
    "\n",
    "    for image in images:\n",
    "        # Convert the image from RGB to HSV color space\n",
    "        image = np.uint8(image)  # Convert to np.uint8 if necessary\n",
    "        hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Extract the hue channel\n",
    "        hue_channel = hsv_image[:,:,0]\n",
    "        \n",
    "        # Calculate statistical measures of the hue values\n",
    "        hue_mean = np.mean(hue_channel)\n",
    "        hue_variance = np.var(hue_channel)\n",
    "        \n",
    "        # Append the hue indicator features to the list\n",
    "        hue_features.append([hue_mean, hue_variance])\n",
    "\n",
    "    # Convert the list to a NumPy array\n",
    "    hue_features = np.array(hue_features)\n",
    "\n",
    "    # Apply feature normalization (e.g., z-score normalization)\n",
    "    hue_features = (hue_features - np.mean(hue_features, axis=0)) / np.std(hue_features, axis=0)\n",
    "\n",
    "    return hue_features\n",
    "\n",
    "\n",
    "def extract_dominant_colors(images, num_colors=1):\n",
    "    dominant_colors = []\n",
    "\n",
    "    for image in images:\n",
    "        # Reshape the image into a 2D array of pixels\n",
    "        pixels = image.reshape(-1, 3)\n",
    "        \n",
    "        # Perform k-means clustering to find dominant colors\n",
    "        _, labels, centers = cv2.kmeans(pixels.astype(np.float32), num_colors, None,\n",
    "                                        criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0),\n",
    "                                        attempts=10, flags=cv2.KMEANS_RANDOM_CENTERS)\n",
    "        \n",
    "        # Convert the centers to uint8 RGB values\n",
    "        colors = centers.astype(np.uint8)\n",
    "        \n",
    "        # Append the dominant color(s) to the list\n",
    "        dominant_colors.append(colors.squeeze(axis=0))\n",
    "\n",
    "    # Convert the list to a NumPy array\n",
    "    dominant_colors = np.array(dominant_colors)\n",
    "\n",
    "    # Apply feature normalization (e.g., min-max normalization)\n",
    "    dominant_colors = (dominant_colors - np.min(dominant_colors, axis=0)) / (np.max(dominant_colors, axis=0) - np.min(dominant_colors, axis=0))\n",
    "\n",
    "    return dominant_colors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 : Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_images(features, num_clusters):\n",
    "    # Perform k-means clustering on the feature vectors\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(features)\n",
    "\n",
    "    # Retrieve the cluster labels assigned to each feature vector\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Return the cluster labels\n",
    "    return labels\n",
    "\n",
    "\n",
    "def kmeans_manhattan(X, n_clusters, max_iter=300, random_state=None):\n",
    "    # Initialize cluster centers randomly\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    indices = rng.choice(len(X), n_clusters, replace=False)\n",
    "    centers = X[indices]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Calculate pairwise Manhattan distances between data points and cluster centers\n",
    "        distances = pairwise_distances_argmin_min(X, centers, metric='manhattan')[0]\n",
    "\n",
    "        # Update cluster centers\n",
    "        new_centers = np.array([X[distances == i].mean(axis=0) for i in range(n_clusters)])\n",
    "\n",
    "        # Check convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "\n",
    "        centers = new_centers\n",
    "\n",
    "    labels = pairwise_distances_argmin_min(X, centers, metric='manhattan')[0]\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def kmeans_mahalanobis(X, n_clusters, max_iter=300, random_state=None):\n",
    "    # Calculate the covariance matrix\n",
    "    cov_matrix = np.cov(X.T)\n",
    "\n",
    "    # Calculate the inverse of the covariance matrix\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "    # Initialize cluster centers using k-means++\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=max_iter, random_state=random_state)\n",
    "    kmeans.fit(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Calculate the Mahalanobis distance between data points and cluster centers\n",
    "        distances = np.array([mahalanobis(x, center, inv_cov_matrix) for x in X for center in centers])\n",
    "\n",
    "        # Reshape the distances into a matrix\n",
    "        distances = distances.reshape(len(X), n_clusters)\n",
    "\n",
    "        # Assign data points to the nearest cluster center\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Update cluster centers\n",
    "        new_centers = np.array([X[labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "\n",
    "        # Check convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "\n",
    "        centers = new_centers\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_optimal_clusters(features, min_clusters, max_clusters):\n",
    "    best_score = -1\n",
    "    best_clusters = None\n",
    "    best_num_clusters = None\n",
    "    \n",
    "    for n_clusters in range(min_clusters, max_clusters):\n",
    "        cluster_labels = kmeans_mahalanobis(features, n_clusters)  # Apply kmeans_mahalanobis\n",
    "        silhouette_avg = silhouette_score(features, cluster_labels)\n",
    "        \n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_clusters = cluster_labels  # Update with the cluster labels from kmeans_mahalanobis\n",
    "            best_num_clusters = n_clusters\n",
    "    \n",
    "    return best_clusters, best_num_clusters\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_puzzle(puzzle):\n",
    "    # Step 1: Determine the dimensions of the output image\n",
    "    if len(puzzle)<9:\n",
    "        return\n",
    "    num_rows = 4\n",
    "    num_cols = 4\n",
    "    if len(puzzle) == 9:\n",
    "        num_rows = 3\n",
    "        num_cols = 3\n",
    "    if len(puzzle) == 12:\n",
    "        num_rows = 3\n",
    "        num_cols = 4\n",
    "    if len(puzzle) == 16:\n",
    "        num_rows = 4\n",
    "        num_cols = 4\n",
    "\n",
    "    # Step 2: Create an empty canvas\n",
    "    canvas_height = 128 * num_rows\n",
    "    canvas_width = 128 * num_cols\n",
    "    canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Step 3: Place each puzzle piece on the canvas\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            if row*num_cols + col >= len(puzzle): return\n",
    "            else : puzzle_piece = puzzle[row*num_cols + col]\n",
    "            if puzzle_piece.shape[0] != 128 or puzzle_piece.shape[1] != 128:\n",
    "                print(\"Error: Invalid puzzle piece shape\")\n",
    "                return None\n",
    "\n",
    "            x_start = col * 128\n",
    "            y_start = row * 128\n",
    "            canvas[y_start:y_start+128, x_start:x_start+128] = puzzle_piece\n",
    "\n",
    "    # Step 4: Save the reconstructed puzzle image\n",
    "    return canvas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT=[]\n",
    "from save_evaluation_files import export_solutions\n",
    "\n",
    "i=0\n",
    "while i < nb_train :\n",
    "    puzzle1 =[]\n",
    "    puzzle2 =[]\n",
    "    puzzle3 =[]\n",
    "    puzzle4 =[]\n",
    "    feature_vectors =[]\n",
    "    cluster =[]\n",
    "\n",
    "    #lpq_features = extract_lpq_features(extracted_piece[i])\n",
    "    lbp_features = extract_lbp_features(extracted_piece[i])\n",
    "    gabor_features = extract_gabor_features(extracted_piece[i])\n",
    "    hue_features = extract_hue_indicator(extracted_piece[i])\n",
    "    dominant_colors = extract_dominant_colors(extracted_piece[i])\n",
    "\n",
    "    #print(gabor_features)\n",
    "    #print(lpq_features)\n",
    "    #print(hue_features)\n",
    "    #print(dominant_colors)\n",
    "\n",
    "    # Combine the feature vectors for each image\n",
    "    feature_vectors = np.concatenate((lbp_features, dominant_colors), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    labels, num_clusters = find_optimal_clusters(feature_vectors, 3, 4)\n",
    "\n",
    "    GT.append(labels)\n",
    "\n",
    "    j=0\n",
    "    while j < len(GT[i]):\n",
    "        if GT[i][j]==0 : puzzle1.append(extracted_piece[i][j])\n",
    "        if GT[i][j]==1 : puzzle2.append(extracted_piece[i][j])\n",
    "        if GT[i][j]==2 : puzzle3.append(extracted_piece[i][j])\n",
    "        if GT[i][j]==3 : puzzle4.append(extracted_piece[i][j])\n",
    "        j+=1\n",
    "\n",
    "    mask = edge_image[i]\n",
    "    solved_puzzle = []\n",
    "    puzzles = [puzzle1, puzzle2, puzzle3, puzzle4]\n",
    "    for puzzle in puzzles:\n",
    "        if len(puzzle)>9 :\n",
    "            cluster.append(puzzle)\n",
    "    reconstructed_puzzles = []\n",
    "    for puzzle in cluster:\n",
    "        im = reconstruct_puzzle(puzzle)\n",
    "        #im = im.astype(np.uint8)  # Convert image data to uint8\n",
    "        reconstructed_puzzles.append(im)\n",
    "\n",
    "    solution = [mask, feature_vectors, puzzles, reconstructed_puzzles] \n",
    "    export_solutions(i,  solution, path = \"data_project\", group_id = \"42\")\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_and_export_puzzles_image(image_index , folder = \"train\" , path = \"data_project\"  , group_id = \"00\"):\n",
    "    \"\"\"\n",
    "    Wrapper funciton to load image and save solution\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    image:\n",
    "        index number of the dataset\n",
    "\n",
    "    Returns\n",
    "    \"\"\"\n",
    "\n",
    "      # open the image\n",
    "    image_loaded = load_input_image(image_index , folder = folder , path = path)\n",
    "    #print(image_loaded)\n",
    "    \n",
    "   \n",
    "    ## call functions to solve image_loaded\n",
    "    solved_puzzles = [ (np.random.rand(512,512,3)*255).astype(np.uint8)  for i in range(2) ]\n",
    "    outlier_images = [ (np.random.rand(128,128,3)*255).astype(np.uint8) for i in range(3)]\n",
    "    \n",
    "    save_solution_puzzles (image_index , solved_puzzles , outlier_images , folder = folder ,group_id =group_id)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    return image_loaded , solved_puzzles , outlier_images\n",
    "\n",
    "im, sol , out = solve_and_export_puzzles_image(6 , group_id = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id = 42\n",
    "# Evaluate all images\n",
    "games_id = [6,10]  # to evaluate  three images\n",
    "\n",
    "for i in games_id :\n",
    "    \n",
    "    print(\"solving \" , i)\n",
    "    # Saving results\n",
    "    solve_and_export_puzzles_image(6 , group_id = group_id)\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "The evaluation metrics will be liberated in the following days. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
